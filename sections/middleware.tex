\section{Data Access and Execution Abstractions}
\label{sec:middleware}

The algorithmic components of the LSST Science Pipelines are built on a suite of packages that together form a powerful data access and execution framework (\texttt{pex\_config}, \texttt{resources}, \texttt{daf\_butler}, \texttt{pipe\_base}, \texttt{ctrl\_mpexec}, and \texttt{ctrl\_bps}).
Unlike most of the rest of the codebase, these packages can be individually installed with \texttt{pip} as well as EUPS and can be used on their own.

\subsection{Butler}

Early in the development of the LSST Science Pipelines software it was decided that the algorithmic code should be written without knowing where files came from, what format they were written in, where the outputs are going to be written or how they are going to be stored.
All that the algorithmic code needs to know is the relevant data model and the Python type.
To meet these requirements we developed a library called the Data Butler \citep[see e.g.,][]{2022SPIE12189E..11J,2023arXiv230303313L}.

The Butler internally is implemented as a registry, a database keeping track of datasets, and a datastore, a storage system that can map a Butler dataset to a specific collection of bytes.
A datastore is usually a file store (including POSIX file system, S3 object stores, or WebDAV) but it is also possible to store metrics directly into the Sasquatch metrics service \citep{SQR-068,2024SPIE13101E..1MF}.

\begin{deluxetable}{ll}

%% Keep a portrait orientation

%% Over-ride the default font size
%% Use Default (12pt)

%% Use \tablewidth{?pt} to over-ride the default table width.
%% If you are unhappy with the default look at the end of the
%% *.log file to see what the default was set at before adjusting
%% this value.

%% This is the title of the table.
\tablecaption{Common dimensions present in the default dimension universe.\label{tab:dims}}

%% This command over-rides LaTeX's natural table count
%% and replaces it with this number.  LaTeX will increment
%% all other tables after this table based on this number
%% \tablenum{1}

%% The \tablehead gives provides the column headers.  It
%% is currently set up so that the column labels are on the
%% top line and the units surrounded by ()s are in the
%% bottom line.  You may add more header information by writing
%% another line between these lines. For each column that requries
%% extra information be sure to include a \colhead{text} command
%% and remember to end any extra lines with \\ and include the
%% correct number of &s.
\tablehead{\colhead{Name} & \colhead{Description} \\
\colhead{} & \colhead{} }

%% All data must appear between the \startdata and \enddata commands
\startdata
\texttt{instrument} &  Instrument.  \\
\texttt{band} & Waveband of interest.  \\
\texttt{physical\_filter} &  Filter used for the exposure. \\
\texttt{day\_obs} & The observing day. \\
\texttt{group} &  Group identifier. \\
\texttt{exposure} & Individual exposure. \\
\texttt{visit} &  Collection of 1 or 2 exposures. \\
\texttt{tract} &  Tesselation of the sky. \\
\texttt{patch} &  Patch within a tract.\\
\enddata

%% Include any \tablenotetext{key}{text}, \tablerefs{ref list},
%% or \tablecomments{text} between the \enddata and
%% \end{deluxetable} commands

\end{deluxetable}

A core concept of the Butler is that every dataset must be given what we call a ``data coordinate.''
The data coordinate locates the dataset in the dimensional space where dimensions are defined in terms that scientists understand.
Some commonly used dimensions are listed in Table~\ref{tab:dims}.
Each dataset is uniquely located by specifying its dataset type, its run collection, and its coordinates, with Butler refusing to accept another dataset that matches all three of those values.
The dataset type defines the relevant dimensions (such as whether this is referring to observations or a sky map) and the associated Python type representing the dataset.
The run collection can be thought of as a folder grouping datasets created by the same batch operation, but does not have to be a folder within a file system.

As a concrete example, the file from one detector of an LSSTCam observation taken sometime in 2025 could have a data coordinate of \texttt{instrument="LSSTCam", detector=42, exposure=2025080300100} and be associated with a \texttt{raw} dataset type.
The \texttt{exposure} record itself implies other information such as the physical filter and the time of observation.
A deep coadd on a patch of sky would not have \texttt{exposure} dimensions at all and would instead be something like \texttt{instrument="LSSTCam", tract=105, patch=2, band="r", skymap="something"}, which would tell you exactly where it is located in the sky and in what waveband since you can calculate it from the tract, patch, band and skymap.

\subsection{Pipelines and Tasks}

The data dimensions system also plays a fundamental role in how the LSST processing pipelines are assembled and run; high-level pieces of algorithmic code called \texttt{PipelineTasks} declare the dimensions of their units of work (``quanta''), their inputs, and their outputs, allowing a directed acyclic graph (a "quantum graph") describing the processing to be assembled from a YAML declaration of the tasks to be run, their configuration, and a Butler database query.
Quantum graphs can range in size from a few tens of quanta (e.g., for the nightly processing performed on a single detector image) to millions (for a piece of the yearly data release pipelines), and serve as the common interface for multiple execution systems, including the low-latency nightly Prompt Processing framework \citep{DMTN-219} and the Batch Processing System \citep[BPS;][]{2022arXiv221115795G}, which adapts quantum graphs for execution at scale by third-party workflow management systems like HTCondor \citep{2022zndo...2579447H}, Parsl \citep{10.1145/3307681.3325400}, and PanDA \citep{2024EPJWC.29504026K}.

Algorithmic code below the \texttt{PipelineTask} level is often subdivided into multiple ``subtasks'' that (like \texttt{PipelineTask} itself) inherit from the base \texttt{Task} class, which provides easy access to hierarchical logging, metadata, and configuration.

\subsection{Pipeline Visualization}
\label{sec:pipeline_visualization}

Visualizing pipeline execution is crucial for understanding task dependencies, debugging, optimizing workflows, and ensuring correct data flow within the LSST Science Pipelines.
To support this, we provide several options for visualizing the pipeline graph (a simplified directed acyclic graph that shows how tasks relate to dataset types but without including data from a Butler query).

Diagrams can be generated as ASCII, useful for a quick inspection from a terminal session, or in graphical formats such as Graphviz DOT\footnote{\url{https://www.graphviz.org}} or Mermaid\footnote{\url{https://mermaid.js.org}}.
The Mermaid format is particularly well-suited for sharing in accessible, web-based contexts.
Figure~\ref{fig:pipe_viz} shows a visualization of a subset of two tasks from the \texttt{LSSTComCam/DRP-v2.yaml} pipeline.
The diagram shows the relationships between tasks and their input and output dataset types as well as the sequence in which the tasks are expected to run.
Such visualizations can help uncover misconfigurations, missing inputs, or unexpected data dependencies that might otherwise result in issues such as empty quantum graphs or failed pipeline execution.
It is also possible to visualize quantum graphs, which include the actual data from a Butler query, but these can be very large and are only useful for small Butler queries to inspect the actual data flow when debugging.

\begin{figure*}
    \centering
    \plotone{figures/middleware/pipe_viz_comcam_subset.pdf}
    \caption{
        Example pipeline visualization of four selected tasks from the \texttt{LSSTComCam/DRP-v2.yaml} pipeline generated using Mermaid.
        The diagram illustrates the flow of datasets between tasks, with dashed lines indicating prerequisite inputs.
        This visualization helps validate task dependencies and the expected sequence of execution.
    }
    \label{fig:pipe_viz}
\end{figure*}

\subsection{Configuration}
\label{sec:config}
The \texttt{pex\_config} package provides the foundational configuration system for running large scale processing campaigns with the science pipelines.
It's far more than a simple parameter parser; it's a framework that mediates between diverse configuration sources and the complex software that processes astronomical data.
At its core, \texttt{pex\_config} functions as an intermediate representation, decoupling the pipelines from the specifics of configuration file formats (like YAML, JSON) and providing a unified, Python-native interface to all configurable parameters.
This intermediate representation, resembling a Domain Specific Language embedded within Python, also allows leveraging the full power of a programming language for parsing or setting configuration values, and includes provenance through parameter history and also configuration validation of both types and values.
The system is flexible enough that it has been adopted by the DRAGONS software \citep{2023RNAAS...7..214L}.
A more detailed description of how the configuration system works is provided in Appendix~\ref{sec:appendix_config}.

\subsection{Instrument Abstractions: Obs Packages}
\label{sec:obs_packages}

The Butler and pipeline construction code know nothing about the specifics of a particular instrument.
In the default dimension universe there is an \texttt{instrument} dimension that includes a field containing the full name of a Python \texttt{Instrument} class.
This class, which uses a standard interface, is used by the system to isolate the instrument-specific from the pipeline-generic.
Some of the responsibilities are:

\begin{itemize}
\item Register instrument-specific dimensions such as \texttt{detector}, \texttt{physical\_filter} and the default \texttt{visit\_system}.
\item Define the default \texttt{raw} dataset type and the associated dimensions.
\item Provide configuration defaults for pipeline task code that is processing data from this instrument.
\item Provide a ``formatter'' class that knows how to read raw data.
\item Define the default curated calibrations known to this instrument.
\end{itemize}

The \texttt{Instrument} interface is defined in two levels: the minimal interface in the \texttt{pipe\_base} package defines everything needed to use the Butler and execution system, while a more complete subclass in the \texttt{obs\_base} package provides considerable additional functionality but is not in the minimal, \texttt{pip}-installable suite because of the additional dependencies on C++ code.

By convention we define the instrument class and associated configuration in \texttt{obs} packages.
There are currently project-supported \texttt{obs} packages for:

\begin{itemize}
\item LSSTCam \citep{10.71929/rubin/2571927,2024SPIE13096E..1SR,2010SPIE.7735E..0JK}, LATISS \citep{10.71929/rubin/2571930,2020SPIE11452E..0UI}, and associated Rubin Observatory test stands, calibration instruments, and simulators.
\item Hyper-SuprimeCam on the Subaru telescope \citep{2018PASJ...70S...1M}.
\item The Dark Energy Camera on the CTIO Blanco telescope \citep{2015AJ....150..150F,2008SPIE.7014E..0ED}.
\item CFHT's MegaPrime \citep{2003SPIE.4841...72B}.
\end{itemize}

Additionally, teams outside the project have developed \texttt{obs} packages to support Subaru's Prime Focus Spectrograph \citep{2020SPIE11447E..7VW}, VISTA's VIRCAM \citep{2015A&A...575A..25S},
the Wide Field Survey Telescope \citep[WFST;][]{2025arXiv250115018C}, and the Gravitational-wave Optical Transient Observer \citep[GOTO;][]{2021PASA...38....4M}.
SPHEREx \citep{2020SPIE11443E..0IC} uses the pipeline middleware but has developed pipeline tasks without the Rubin C++ dependencies and so does not use an \texttt{obs} package.

\subsection{Metadata Translation}

Every instrument uses different metadata standards but the Butler data model and pipelines require some form of standardization to determine values such as the coordinates of an observation, the observation type, or the time of observation.
To perform that standard extraction of metadata each supported instrument must provide a metadata translator class using the \texttt{astro\_metadata\_translator} infrastructure.\footnote{\url{https://astro-metadata-translator.lsst.io}}
The translator classes can understand evolving data models and allow the standardized metadata to be extracted for the lifetime of an instrument even if headers changed.
Furthermore, in addition to providing standardized metadata the package can also provide programmatic or per-exposure corrections to data headers prior to calculating the translated metadata.
This allows files that were written with incorrect headers to be recovered during file ingestion.
